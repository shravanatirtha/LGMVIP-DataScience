<NeuralDesignerOutput>
 <Task Component="Training strategy" Id="aaZV1B" Name="Report training strategy" Title="Training strategy">
  <Text Id="9wl63j" Title="Training strategy">The procedure used to carry out the learning process is called training (or learning) strategy.
The training strategy is applied to the neural network in order to obtain the best possible loss.
</Text>
  <Text Id="dIOzlY" Title="Loss index">The loss index defines the task the neural network is requiered to do and provides a measure of the quality of the representation requiered to learn.
When setting a loss index, two different terms must be chosen: an error term and a regularization term. 

The error term evaluates quantitatively how the neural network fits the data set. There are several error methods, and the choice of an appropriate one depends on the particular application. In this case, the Normalized Squared Error (MSE) is selected. The normalized squared error has a value of one when the outputs from the neural network are equal to the mean values of the target variables, while a value of zero means perfect prediction of the data. 

 The regularization term measures the values of the parameters in the neural network. Adding it to the error will cause the neural network to have smaller weights and biases, which will force its response to be smoother, i.e., to avoid overfitting. In this case, L2 regularization method is applied. It consists of the squared sum of all the parameters in the neural network. </Text>
  <Table Id="hdUGWn" Title="Optimization algorithm">
   <Caption Id="3eOfZd">The quasi-Newton method is used here as optimization algorithm.
It is based on Newton's method, but does not require calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm, by only using gradient information. </Caption>
   <Data>Method used to obtain a suitable training rate.\BFGS
Method used to calculate the step for the quasi-Newton training direction.\BrentMethod
Maximum interval length for the learning rate.\0.001000
Minimum loss improvement between two successive epochs.\0.000000
Goal value for the loss.\0.001000
Goal value for the norm of the objective function gradient.\100
Maximum number of epochs at which the selection error increases.\1000
Maximum number of epochs to perform the training.\01:00:00</Data>
   <RowsName>Inverse hessian approximation method\Learning rate method\Learning rate tolerance\Minimum loss decrease\Loss goal\Maximum selection error increases\Maximum epochs number\Maximum time</RowsName>
   <ColumnsName>Description\Value</ColumnsName>
   <RowHeadingsWidth>27</RowHeadingsWidth>
   <ColumnHeadingsWidth>25</ColumnHeadingsWidth>
   <Alignment>center</Alignment>
  </Table>
 </Task>
 <Task Component="Data set" Id="lFKsNt" Name="Calculate columns distribution" Title="Data distribution">
  <Text Id="QcuyD5" Title="Task description">Histograms show how the data is distributed over its entire range.
In classification problems, a uniform distribution for all the variables is, in general, desirable.
If the data is very irregularly distributed, then the model will probably be of bad quality. </Text>
  <HistogramChart Id="tQ9lzN" Title="sepal_length distribution">
   <Caption Id="jgqNcg">The following chart shows the histogram for the variable sepal_length.
 The abscissa represents the centers of the containers, and the ordinate their corresponding frequencies.
The maximum frequency is 18%, which corresponds to the bin with center 5.56. 
The minimum frequency is 3.33333%, which corresponds to the bin with center 7.36. </Caption>
   <Name>sepal_length</Name>
   <NominalNames/>
   <Centers>4.48\4.84\5.2\5.56\5.92\6.28\6.64\7\7.36\7.72</Centers>
   <Frequencies>6\15.3\9.33\18\14.7\13.3\12\4\3.33\4</Frequencies>
   <Minimums>4.3\4.66\5.02\5.38\5.74\6.1\6.46\6.82\7.18\7.54</Minimums>
   <Maximums>4.66\5.02\5.38\5.74\6.1\6.46\6.82\7.18\7.54\7.9</Maximums>
   <Minimum>0</Minimum>
   <Maximum>110</Maximum>
  </HistogramChart>
  <HistogramChart Id="dEQ5n8" Title="sepal_width distribution">
   <Caption Id="k1es9S">The following chart shows the histogram for the variable sepal_width.
 The abscissa represents the centers of the containers, and the ordinate their corresponding frequencies.
The maximum frequency is 25.3333%, which corresponds to the bin with center 3.08. 
The minimum frequency is 1.33333%, which corresponds to the bins with centers 4.04, 4.28. </Caption>
   <Name>sepal_width</Name>
   <NominalNames/>
   <Centers>2.12\2.36\2.6\2.84\3.08\3.32\3.56\3.8\4.04\4.28</Centers>
   <Frequencies>2.67\4.67\14.7\16\25.3\20.7\6\7.33\1.33\1.33</Frequencies>
   <Minimums>2\2.24\2.48\2.72\2.96\3.2\3.44\3.68\3.92\4.16</Minimums>
   <Maximums>2.24\2.48\2.72\2.96\3.2\3.44\3.68\3.92\4.16\4.4</Maximums>
   <Minimum>0</Minimum>
   <Maximum>110</Maximum>
  </HistogramChart>
  <HistogramChart Id="hZ2w3d" Title="petal_length distribution">
   <Caption Id="Hw1POi">The following chart shows the histogram for the variable petal_length.
 The abscissa represents the centers of the containers, and the ordinate their corresponding frequencies.
The maximum frequency is 24.6667%, which corresponds to the bin with center 1.3. 
The minimum frequency is 0%, which corresponds to the bin with center 2.47. </Caption>
   <Name>petal_length</Name>
   <NominalNames/>
   <Centers>1.3\1.88\2.47\3.07\3.66\4.25\4.84\5.43\6.02\6.61</Centers>
   <Frequencies>24.7\8.67\0\2\5.33\17.3\19.3\12\7.33\3.33</Frequencies>
   <Minimums>1\1.59\2.18\2.77\3.36\3.95\4.54\5.13\5.72\6.31</Minimums>
   <Maximums>1.59\2.18\2.77\3.36\3.95\4.54\5.13\5.72\6.31\6.9</Maximums>
   <Minimum>0</Minimum>
   <Maximum>110</Maximum>
  </HistogramChart>
  <HistogramChart Id="3F0A2m" Title="petal_width distribution">
   <Caption Id="lpjuJ1">The following chart shows the histogram for the variable petal_width.
 The abscissa represents the centers of the containers, and the ordinate their corresponding frequencies.
The maximum frequency is 27.3333%, which corresponds to the bin with center 0.22. 
The minimum frequency is 0.666667%, which corresponds to the bin with center 0.7. </Caption>
   <Name>petal_width</Name>
   <NominalNames/>
   <Centers>0.22\0.46\0.7\0.94\1.18\1.42\1.66\1.9\2.14\2.38</Centers>
   <Frequencies>27.3\5.33\0.667\4.67\14\13.3\4\15.3\6\9.33</Frequencies>
   <Minimums>0.1\0.34\0.58\0.82\1.06\1.3\1.54\1.78\2.02\2.26</Minimums>
   <Maximums>0.34\0.58\0.82\1.06\1.3\1.54\1.78\2.02\2.26\2.5</Maximums>
   <Minimum>0</Minimum>
   <Maximum>110</Maximum>
  </HistogramChart>
  <PieChart Id="U8XjjD" Title="class distribution pie chart">
   <Caption Id="OraOli">The following pie chart shows the distribution for the categorical variable class, which contains 3 variables: iris_setosa, iris_versicolor and iris_virginica.
 The minimum frequency is 33.3333%, which corresponds to the categories iris_setosa, iris_setosa and iris_versicolor.
The maximum frequency is 33.3333%, which corresponds to the categories iris_setosa, iris_setosa and iris_versicolor.</Caption>
   <Data>33.3333\33.3333\33.3333</Data>
   <Names Id="zwBvIc">iris_setosa\iris_versicolor\iris_virginica</Names>
  </PieChart>
 </Task>
 <Task Component="Neural network" Id="Q1bETo" Name="Report neural network" Title="Neural network">
  <Text Id="sks63t" Title="Task description">The neural network represents the predictive model. In Neural Designer neural networks allow deep architectures, which are a class of universal approximator. </Text>
  <Table Id="oZ8AP9" Title="Inputs">
   <Caption Id="XZ2gSY">The number of inputs is 4.
The next table depicts the names of the inputs to the neural network.
</Caption>
   <Data>sepal_length
sepal_width
petal_length
 petal_width</Data>
   <RowsName>1\2\3\4</RowsName>
   <ColumnsName>  Name   </ColumnsName>
   <RowHeadingsWidth>3</RowHeadingsWidth>
   <ColumnHeadingsWidth>12</ColumnHeadingsWidth>
   <Alignment>left</Alignment>
  </Table>
  <Table Id="mvQfPK" Title="Scaling layer">
   <Caption Id="Bm0oNV">The size of the scaling layer is 4, the number of inputs. The following table shows the values which are used for scaling the inputs, which include the minimum, maximum, mean and standard deviation. </Caption>
   <Data>4.3\7.9\5.84333\0.825301\MeanStandardDeviation
2\4.4\3.054\0.432147\MeanStandardDeviation
1\6.9\3.75867\1.75853\MeanStandardDeviation
0.1\2.5\1.19867\0.760613\MeanStandardDeviation</Data>
   <RowsName>sepal_length\sepal_width\petal_length\ petal_width</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation\Scaler</ColumnsName>
   <RowHeadingsWidth>9</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="KSStL7" Title="Perceptron layers">
   <Caption Id="IjVOmW">The number of perceptron layers in the neural network is 1. The following table depicts the size of each layer and its corresponding activation function. </Caption>
   <Data>4\3\HyperbolicTangent</Data>
   <RowsName>1</RowsName>
   <ColumnsName>Inputs number\Neurons number\Activation function</ColumnsName>
   <RowHeadingsWidth>5</RowHeadingsWidth>
   <ColumnHeadingsWidth>14</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="qRtzrf" Title="Probabilistic layer">
   <Caption Id="cOI0r4">The following table depicts the size of the probabilistic layer the corresponding activation function. </Caption>
   <Data>3\3\Softmax</Data>
   <RowsName>1</RowsName>
   <ColumnsName>Inputs number\Neurons number\Activation function</ColumnsName>
   <RowHeadingsWidth>5</RowHeadingsWidth>
   <ColumnHeadingsWidth>14</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Hpau0j" Title="Outputs table">
   <Caption Id="dl5jfI">The number of outputs is 3. The next table depicts the name of the outputs from the neural network.
</Caption>
   <Data>iris_setosa
iris_versicolor
iris_virginica</Data>
   <RowsName>1\2\3</RowsName>
   <ColumnsName>  Name   </ColumnsName>
   <RowHeadingsWidth>3</RowHeadingsWidth>
   <ColumnHeadingsWidth>12</ColumnHeadingsWidth>
   <Alignment>left</Alignment>
  </Table>
  <NeuralNetworkGraph Id="Pfbuz8" Title="Network architecture">
   <Caption Id="3nL8Q3">A graphical representation of the resulted deep architecture is depicted next.
It contains the following layers:
-Scaling layer with 4 neurons (yellow).
-Perceptron layer with 3 neurons (blue).
-Probabilistic layer with 3 neurons (red).
</Caption>
   <ProjectType>classification</ProjectType>
   <InputsName>sepal_length\sepal_width\petal_length\ petal_width</InputsName>
   <OutputsName>iris_setosa\iris_versicolor\iris_virginica</OutputsName>
   <LayersName>scaling_layer\perceptron_layer_1\probabilistic_layer</LayersName>
   <Architecture>4\3\3</Architecture>
  </NeuralNetworkGraph>
 </Task>
 <Task Component="Training strategy" Id="e9mDbu" Name="Perform training" Title="Training">
  <Text Id="rmxR4K" Title="Task description">The procedure used to carry out the learning process is called training (or learning) strategy.
The training strategy is applied to the neural network in order to obtain the best possible loss.
The type of training is determined by the way in which the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="F6Im1i" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method, but does not require calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm, by only using gradient information. </Text>
  <DoubleLineChart Id="kqPIjY" Title="Quasi-Newton method errors history">
   <Caption Id="6lmp9d">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error and the orange line represents the selection error.
The initial value of the training error is 0.0250907, and the final value after 3 epochs is 0.0250907.
The initial value of the selection error is 0.036931, and the final value after 3 epochs is 0.0369243.
</Caption>
   <X1Data>0\1\2\3</X1Data>
   <X2Data>0\1\2\3</X2Data>
   <Y1Data>0.0251\0.0251\0.0251\0.0251</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.0369\0.0369\0.0369\0.0369</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>4</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>1</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="5mHTNf" Title="Quasi-Newton method results">
   <Caption Id="NKNTOb">The next table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index and the optimization algorithm.
</Caption>
   <Data>0.0251
0.0369
3
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Component="Model selection" Id="E2dveA" Name="Perform neurons selection" Title="Order selection">
  <Text Id="myJOFT" Title="Task description">The best selection is achieved by using a model whose complexity is the most appropriate to produce an adequate fit of the data.
The neurons selection algorithm is responsible of finding the optimal number of neurons in the network. </Text>
  <Text Id="ru0x79" Title="Neurons selection algorithm">The growing neurons algorithm is used here to select the optimal order in the neural network. </Text>
  <DoubleLineChart Id="0Yvuej" Title="Growing neurons training/selection errors plot">
   <Caption Id="uYytsB">The next chart shows the error history for the different subsets during the growing neurons selection process.
The blue line represents the training error and the yellow line symbolizes the selection error. </Caption>
   <X1Data>1\2\3\4\5\6\7\8\9\10</X1Data>
   <X2Data>1\2\3\4\5\6\7\8\9\10</X2Data>
   <Y1Data>0.0646037\0.0253638\0.0250919\0.017345\0.0256796\0.0169735\0.0169445\0.0169083\0.0159684\0.0168634</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.0576488\0.0374561\0.0369228\0.0383795\0.0365032\0.0407835\0.0407225\0.0407117\0.0420499\0.0406739</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Neurons number</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>1</XMinimum>
   <XMaximum>10</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>1</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="nFvdcv" Title="Growing neurons results">
   <Caption Id="zYzt6A">The next table shows the neurons selection results by the growing neurons algorithm.
They include some final states from the neural network, the error functional and the neurons selection algorithm. </Caption>
   <Data>5
0.0256796
0.0365032
10
00:00:01
</Data>
   <RowsName>Optimal order\Optimum training error\Optimum selection error\Epochs number\Elapsed time</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>17</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <NeuralNetworkGraph Id="WdRdqf" Title="Network architecture">
   <Caption Id="2d0i4J">A graphical representation of the resulted deep architecture is depicted next.
It contains the following layers:
-Scaling layer with 4 neurons (yellow).
-Perceptron layer with 5 neurons (blue).
-Probabilistic layer with 3 neurons (red).
</Caption>
   <ProjectType>classification</ProjectType>
   <InputsName>sepal_length\sepal_width\petal_length\petal_width</InputsName>
   <OutputsName>iris_setosa\iris_versicolor\iris_virginica</OutputsName>
   <LayersName>scaling_layer\perceptron_layer_1\probabilistic_layer</LayersName>
   <Architecture>4\5\3</Architecture>
  </NeuralNetworkGraph>
 </Task>
 <Task Component="Training strategy" Id="O5jUHb" Name="Perform training" Title="Training">
  <Text Id="77Aeab" Title="Task description">The procedure used to carry out the learning process is called training (or learning) strategy.
The training strategy is applied to the neural network in order to obtain the best possible loss.
The type of training is determined by the way in which the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="h7dxDd" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method, but does not require calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm, by only using gradient information. </Text>
  <DoubleLineChart Id="TG5Oah" Title="Quasi-Newton method errors history">
   <Caption Id="FH6Zoc">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error and the orange line represents the selection error.
The initial value of the training error is 0.0256796, and the final value after 8 epochs is 0.0256815.
The initial value of the selection error is 0.0365033, and the final value after 8 epochs is 0.0365064.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8</X2Data>
   <Y1Data>0.0257\0.0257\0.0257\0.0257\0.0257\0.0257\0.0257\0.0257\0.0257</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.0365\0.0365\0.0365\0.0365\0.0365\0.0365\0.0365\0.0365\0.0365</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata></Metadata>
   <MetadataName></MetadataName>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>9</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>1</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="O00C7Y" Title="Quasi-Newton method results">
   <Caption Id="MEWesB">The next table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index and the optimization algorithm.
</Caption>
   <Data>0.0257
0.0365
8
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
</NeuralDesignerOutput>
